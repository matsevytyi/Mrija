{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0MR3cIRDH56"
      },
      "outputs": [],
      "source": [
        "# Neural-Collaborative Filtering Approach\n",
        "\n",
        "# idea - instead of matrix factorisation and dot products, train MLP or NeuMF approach\n",
        "# paper link - https://arxiv.org/pdf/1708.05031\n",
        "\n",
        "# application - integrate the current engine as a part of post-feedback evaluation pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HGCWAxeqDwoC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uJSyQ968DE6k"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(user_input, answer):\n",
        "\n",
        "    le_user = LabelEncoder()\n",
        "    le_profession = LabelEncoder()\n",
        "\n",
        "    users = [user_input] * len(answer)  # user input as one user interacting with multiple professions\n",
        "    professions = [item[0] for item in answer]  # obtain profession names\n",
        "\n",
        "    user_ids = le_user.fit_transform(users)\n",
        "    profession_ids = le_profession.fit_transform(professions)\n",
        "    scores = torch.tensor([item[1] for item in answer], dtype=torch.float32)  # original scores\n",
        "\n",
        "    return user_ids, profession_ids, scores, le_user, le_profession"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QiXa0NgcD_FS"
      },
      "outputs": [],
      "source": [
        "# GMF-based solution\n",
        "\n",
        "class GMF(nn.Module):\n",
        "\n",
        "    def __init__(self, num_users, num_items, embedding_dim):\n",
        "\n",
        "        super(GMF, self).__init__()\n",
        "\n",
        "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
        "        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n",
        "        self.output_layer = nn.Linear(embedding_dim, 1)\n",
        "\n",
        "    def forward(self, user, item):\n",
        "\n",
        "        user_emb = self.user_embedding(user)\n",
        "        item_emb = self.item_embedding(item)\n",
        "\n",
        "        interaction = user_emb * item_emb\n",
        "\n",
        "        return torch.sigmoid(self.output_layer(interaction))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wgqfXTMmEEX2"
      },
      "outputs": [],
      "source": [
        "# MLP-based solution\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, num_users, num_items, embedding_dim, hidden_layers):\n",
        "        super(MLP, self).__init__()\n",
        "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
        "        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n",
        "\n",
        "        # FC layers\n",
        "        layers = []\n",
        "        input_size = embedding_dim * 2  # concatenate embeddings\n",
        "\n",
        "        for layer_size in hidden_layers:\n",
        "            layers.append(nn.Linear(input_size, layer_size))\n",
        "            layers.append(nn.ReLU())\n",
        "            input_size = layer_size\n",
        "\n",
        "        layers.append(nn.Linear(input_size, 1))  # output layer\n",
        "        self.fc_layers = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, user, item):\n",
        "\n",
        "        # concatenate embeddings\n",
        "        user_emb = self.user_embedding(user)\n",
        "        item_emb = self.item_embedding(item)\n",
        "\n",
        "        x = torch.cat([user_emb, item_emb], dim=-1)\n",
        "\n",
        "        return torch.sigmoid(self.fc_layers(x))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "IcuWmQ-pE9Ib"
      },
      "outputs": [],
      "source": [
        "class NeuMF(nn.Module):\n",
        "\n",
        "    def __init__(self, num_users, num_items, embedding_dim, hidden_layers):\n",
        "\n",
        "        super(NeuMF, self).__init__()\n",
        "\n",
        "        # GMF part\n",
        "        self.user_embedding_gmf = nn.Embedding(num_users, embedding_dim)\n",
        "        self.item_embedding_gmf = nn.Embedding(num_items, embedding_dim)\n",
        "\n",
        "        # MLP cpart\n",
        "        self.user_embedding_mlp = nn.Embedding(num_users, embedding_dim)\n",
        "        self.item_embedding_mlp = nn.Embedding(num_items, embedding_dim)\n",
        "\n",
        "        # FC for MLP\n",
        "        layers = []\n",
        "        input_size = embedding_dim * 2\n",
        "\n",
        "        for layer_size in hidden_layers:\n",
        "            layers.append(nn.Linear(input_size, layer_size))\n",
        "            layers.append(nn.ReLU())\n",
        "            input_size = layer_size\n",
        "\n",
        "        self.mlp_fc_layers = nn.Sequential(*layers)\n",
        "\n",
        "        # final layer (conbines both approaches)\n",
        "        self.output_layer = nn.Linear(embedding_dim + hidden_layers[-1], 1)\n",
        "\n",
        "    def forward(self, user, item):\n",
        "\n",
        "        # GMF forward pass\n",
        "        user_emb_gmf = self.user_embedding_gmf(user)\n",
        "        item_emb_gmf = self.item_embedding_gmf(item)\n",
        "        gmf_output = user_emb_gmf * item_emb_gmf  # Element-wise product\n",
        "\n",
        "        # MLP forward pass\n",
        "        user_emb_mlp = self.user_embedding_mlp(user)\n",
        "        item_emb_mlp = self.item_embedding_mlp(item)\n",
        "        mlp_input = torch.cat([user_emb_mlp, item_emb_mlp], dim=-1)\n",
        "        mlp_output = self.mlp_fc_layers(mlp_input)\n",
        "\n",
        "        # concatenate outputs\n",
        "        combined = torch.cat([gmf_output, mlp_output], dim=-1)\n",
        "        return torch.sigmoid(self.output_layer(combined))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-l-8LTrxFFBp"
      },
      "outputs": [],
      "source": [
        "def train_model(model, user_ids, profession_ids, scores, num_epochs=10, lr=0.001):\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.BCELoss()\n",
        "\n",
        "    user_ids = torch.tensor(user_ids, dtype=torch.long)\n",
        "    profession_ids = torch.tensor(profession_ids, dtype=torch.long)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "      # forward pass\n",
        "        model.train()\n",
        "        outputs = model(user_ids, profession_ids).squeeze()\n",
        "        loss = criterion(outputs, scores)\n",
        "\n",
        "        # backward propagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "APHDRuyIFryG"
      },
      "outputs": [],
      "source": [
        "def predict_top_professions(model, user_input, answer, le_user, le_profession, top_k=20):\n",
        "\n",
        "    user_id = le_user.transform([user_input])[0]  # encode user input\n",
        "    professions = [item[0] for item in answer]\n",
        "    profession_ids = le_profession.transform(professions)  # encode professions\n",
        "\n",
        "    user_ids = torch.tensor([user_id] * len(profession_ids), dtype=torch.long)\n",
        "    profession_ids = torch.tensor(profession_ids, dtype=torch.long)\n",
        "\n",
        "    # obtain scores\n",
        "    with torch.no_grad():\n",
        "        predictions = model(user_ids, profession_ids).squeeze().numpy()\n",
        "\n",
        "    # sort\n",
        "    ranked_professions = sorted(zip(professions, predictions), key=lambda x: x[1], reverse=True)\n",
        "    return ranked_professions[:top_k]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "fSA9WJorFxCC"
      },
      "outputs": [],
      "source": [
        "# sample data\n",
        "\n",
        "#TODO: upsample the co0llected feedback dataset and train with it\n",
        "\n",
        "#TODO: change to the real feedback pipeline output\n",
        "\n",
        "#TODO: integrate in the pipeline\n",
        "\n",
        "\n",
        "\n",
        "user_input = \"math and humanities\"\n",
        "answer = [(\"Scientist\", 0.8), (\"Historian\", 0.6), (\"Engineer\", 0.9), (\"Teacher\", 0.7)]\n",
        "\n",
        "user_ids, profession_ids, scores, le_user, le_profession = preprocess_data(user_input, answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2D8JIr8FSNJ"
      },
      "outputs": [],
      "source": [
        "# Train and evaluate GMF\n",
        "gmf_model = GMF(num_users=len(set(user_ids)),\n",
        "                num_items=len(set(profession_ids)),\n",
        "                embedding_dim=16\n",
        "                )\n",
        "\n",
        "train_model(gmf_model, user_ids, profession_ids, scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHIOuLy1FT2K"
      },
      "outputs": [],
      "source": [
        "# Train and evaluate MLP\n",
        "mlp_model = MLP(num_users=len(set(user_ids)),\n",
        "                num_items=len(set(profession_ids)),\n",
        "                embedding_dim=16,\n",
        "                hidden_layers=[64, 32]\n",
        "                )\n",
        "\n",
        "train_model(mlp_model, user_ids, profession_ids, scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6w6yFKzzFU7T"
      },
      "outputs": [],
      "source": [
        "# Train and evaluate NeuMF\n",
        "neumf_model = NeuMF(num_users=len(set(user_ids)),\n",
        "                    num_items=len(set(profession_ids)),\n",
        "                    embedding_dim=16,\n",
        "                    hidden_layers=[64, 32]\n",
        "                    )\n",
        "\n",
        "train_model(neumf_model, user_ids, profession_ids, scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddFZOhK7GUO9",
        "outputId": "2706b13c-4c80-46b5-a98e-7d0b06ff1f7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GMF predictions:\n",
            "Scientist: 0.6226\n",
            "Historian: 0.5572\n",
            "Engineer: 0.3693\n",
            "Teacher: 0.3217\n"
          ]
        }
      ],
      "source": [
        "print(\"GMF predictions:\")\n",
        "\n",
        "top_20_professions = predict_top_professions(gmf_model, user_input, answer, le_user, le_profession)\n",
        "\n",
        "for profession, score in top_20_professions:\n",
        "    print(f\"{profession}: {score:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yr-VYupQGiwh",
        "outputId": "78d73826-efa0-4c6b-def4-ae20b3c7ff1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLP predictions:\n",
            "Scientist: 0.6054\n",
            "Teacher: 0.5804\n",
            "Engineer: 0.5783\n",
            "Historian: 0.5498\n"
          ]
        }
      ],
      "source": [
        "print(\"MLP predictions:\")\n",
        "top_20_professions = predict_top_professions(mlp_model, user_input, answer, le_user, le_profession)\n",
        "\n",
        "for profession, score in top_20_professions:\n",
        "    print(f\"{profession}: {score:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ti7xYPvCGkMT",
        "outputId": "27665d95-9974-4aa9-c236-de3fe61841a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Combined NeuMF predictions:\n",
            "Scientist: 0.7331\n",
            "Engineer: 0.6927\n",
            "Teacher: 0.6816\n",
            "Historian: 0.6450\n"
          ]
        }
      ],
      "source": [
        "print(\"Combined NeuMF predictions:\")\n",
        "\n",
        "top_20_professions = predict_top_professions(neumf_model, user_input, answer, le_user, le_profession)\n",
        "\n",
        "for profession, score in top_20_professions:\n",
        "    print(f\"{profession}: {score:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
