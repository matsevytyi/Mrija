{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HGCWAxeqDwoC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uJSyQ968DE6k"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(user_input, answer):\n",
        "\n",
        "    le_user = LabelEncoder()\n",
        "    le_profession = LabelEncoder()\n",
        "\n",
        "    users = [user_input] * len(answer)  # user input as one user interacting with multiple professions\n",
        "    professions = [item[0] for item in answer]  # obtain profession names\n",
        "\n",
        "    user_ids = le_user.fit_transform(users)\n",
        "    profession_ids = le_profession.fit_transform(professions)\n",
        "    scores = torch.tensor([item[1] for item in answer], dtype=torch.float32)  # original scores\n",
        "\n",
        "    return user_ids, profession_ids, scores, le_user, le_profession"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QiXa0NgcD_FS"
      },
      "outputs": [],
      "source": [
        "# GMF-based solution\n",
        "\n",
        "class GMF(nn.Module):\n",
        "\n",
        "    def __init__(self, num_users, num_items, embedding_dim):\n",
        "\n",
        "        super(GMF, self).__init__()\n",
        "\n",
        "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
        "        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n",
        "        self.output_layer = nn.Linear(embedding_dim, 1)\n",
        "\n",
        "    def forward(self, user, item):\n",
        "\n",
        "        user_emb = self.user_embedding(user)\n",
        "        item_emb = self.item_embedding(item)\n",
        "\n",
        "        interaction = user_emb * item_emb\n",
        "\n",
        "        return torch.sigmoid(self.output_layer(interaction))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-l-8LTrxFFBp"
      },
      "outputs": [],
      "source": [
        "def train_model(model, user_ids, profession_ids, scores, num_epochs=10, lr=0.001):\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.BCELoss()\n",
        "\n",
        "    user_ids = torch.tensor(user_ids, dtype=torch.long)\n",
        "    profession_ids = torch.tensor(profession_ids, dtype=torch.long)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "      # forward pass\n",
        "        model.train()\n",
        "        outputs = model(user_ids, profession_ids).squeeze()\n",
        "        loss = criterion(outputs, scores)\n",
        "\n",
        "        # backward propagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "APHDRuyIFryG"
      },
      "outputs": [],
      "source": [
        "def predict_top_professions(model, user_input, answer, le_user, le_profession, top_k=20):\n",
        "\n",
        "    user_id = le_user.transform([user_input])[0]  # encode user input\n",
        "    professions = [item[0] for item in answer]\n",
        "    profession_ids = le_profession.transform(professions)  # encode professions\n",
        "\n",
        "    user_ids = torch.tensor([user_id] * len(profession_ids), dtype=torch.long)\n",
        "    profession_ids = torch.tensor(profession_ids, dtype=torch.long)\n",
        "\n",
        "    # obtain scores\n",
        "    with torch.no_grad():\n",
        "        predictions = model(user_ids, profession_ids).squeeze().numpy()\n",
        "\n",
        "    # sort\n",
        "    ranked_professions = sorted(zip(professions, predictions), key=lambda x: x[1], reverse=True)\n",
        "    return ranked_professions[:top_k]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "fSA9WJorFxCC"
      },
      "outputs": [],
      "source": [
        "# sample data, change to the real feedback pipeline output\n",
        "\n",
        "user_input = \"math and humanities\"\n",
        "answer = [(\"Scientist\", 0.8), (\"Historian\", 0.6), (\"Engineer\", 0.9), (\"Teacher\", 0.7)]\n",
        "\n",
        "user_ids, profession_ids, scores, le_user, le_profession = preprocess_data(user_input, answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2D8JIr8FSNJ"
      },
      "outputs": [],
      "source": [
        "# Train and evaluate GMF\n",
        "gmf_model = GMF(num_users=len(set(user_ids)),\n",
        "                num_items=len(set(profession_ids)),\n",
        "                embedding_dim=16\n",
        "                )\n",
        "\n",
        "train_model(gmf_model, user_ids, profession_ids, scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddFZOhK7GUO9",
        "outputId": "2706b13c-4c80-46b5-a98e-7d0b06ff1f7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GMF predictions:\n",
            "Scientist: 0.6226\n",
            "Historian: 0.5572\n",
            "Engineer: 0.3693\n",
            "Teacher: 0.3217\n"
          ]
        }
      ],
      "source": [
        "print(\"GMF predictions:\")\n",
        "\n",
        "top_20_professions = predict_top_professions(gmf_model, user_input, answer, le_user, le_profession)\n",
        "\n",
        "for profession, score in top_20_professions:\n",
        "    print(f\"{profession}: {score:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
